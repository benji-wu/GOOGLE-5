# ğŸš€ ç¬¬ä¸€æ¬¡ä½¿ç”¨è«‹å…ˆåŸ·è¡Œä»¥ä¸‹å®‰è£ï¼š
# pip install pandas numpy nltk sentence-transformers

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from sentence_transformers import SentenceTransformer

# ğŸ“Œ NLTK åˆå§‹åŒ–ï¼ˆåƒ…éœ€ç¬¬ä¸€æ¬¡ï¼‰
nltk.download('punkt')
nltk.download('stopwords')

# === ğŸ“ 1. è®€å–èˆ‡æ¸…ç†è³‡æ–™ ===
# è«‹ä¸‹è¼‰ä¸¦ç¢ºèªæª”æ¡ˆè·¯å¾‘ç‚º IMDb_Top_1000.csv
df = pd.read_csv("IMDb_Top_1000.csv")

# é¸å–æ¬„ä½ä¸¦é‡æ–°å‘½å
df = df[['Series_Title', 'Genre', 'Overview']].rename(columns={
    'Series_Title': 'Title',
    'Overview': 'Plot'
})

# æ¸…ç†æ–‡å­—å…§å®¹ï¼ˆå»é™¤åœç”¨è©èˆ‡æ¨™é»ï¼‰
stop_words = set(stopwords.words('english'))

def preprocess(text):
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    return " ".join(tokens)

df['Clean_Plot'] = df['Plot'].astype(str).apply(preprocess)

# å„²å­˜æ¸…ç†å¾Œçš„ CSVï¼ˆå¯é¸ï¼‰
df.to_csv("clean_movie_dataset.csv", index=False)
print("âœ… Cleaned movie dataset saved as 'clean_movie_dataset.csv'")

# === ğŸ¤– 2. ä½¿ç”¨ SentenceTransformer ç”¢ç”Ÿå‘é‡åµŒå…¥ ===
# å–å‰ 50 ç­†åŠ‡æƒ…æ‘˜è¦ï¼ˆç¬¦åˆä»»å‹™æœ€ä½éœ€æ±‚ï¼‰
plots = df['Clean_Plot'][:50].tolist()

# è¼‰å…¥é è¨“ç·´æ¨¡å‹ï¼ˆ384 ç¶­åº¦è¼¸å‡ºï¼‰
model = SentenceTransformer('all-MiniLM-L6-v2')

# åŸ·è¡Œè½‰æ›
embeddings = model.encode(plots, show_progress_bar=True)

# å„²å­˜ç‚º .npy æ ¼å¼ä»¥ä¾¿å¾ŒçºŒæ¨è–¦ä½¿ç”¨
np.save("movie_embeddings.npy", embeddings)
print("âœ… Embeddings generated and saved as 'movie_embeddings.npy'")

